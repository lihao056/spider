### 爬虫入门记录

#### 1.简单的爬虫

​	每一个网站都会有名为robot.txt的文件用于规定是否有禁止访客获取的数据，对于没有的到允许的用户，不能爬取数据。

##### 1.1 网页结构

​	网页其实就是一个框架，网页的标题，链接都是放置在网页源码中。

​	可以使用[Ctrl + U]打开源码界面，查看网页源码

###### 1.1.1 网页结构组成

​	1.HTML

​	HTML相当于整个网页的结构，带<>的符号都是属于HTML的标签。

​	2.CSS

​	CSS表示样式，在CSS中定义了外观。

​	3.JScript

​	JScript表示功能，交互的内容都在JScript中，描述了网站的各种功能

###### 1.1.2 写一个简单的HTML

​	可以通过记事本写入一下内容

```html
<html>
<head>
	<title> title</title>
</head>
<body>
	<div>
		<p>内容</p>
	</div>
	<div>
		<ul>
			<li> <a href="http://www.baidu.com">爬虫</a></li>
			<li> 数据清洗</li>
		</ul>
	</div>
</body>
</html>
```

​	保存后再修改.txt后缀名为.html，打开即可

![1555589188334](https://github.com/lihao056/spider/blob/master/book_learning/picture/1555589188334.png)

​	这是我们写的网页，分别对应于<title>, <p>, <li>三个标题。

##### 1.2 使用request库请求网站

**爬虫的基本原理**

​	1.Resquest(请求)：向服务器发送访问请求。

​	2.Response(响应)：服务器在接收到用户的请求后，会验证请求的有效性，并向用户(客户端)发送响应请求。

**网页(http)的请求方式**

​	GET：最常见的请求方式，把参数包含在URL中。

​	POST：需要通过另外步骤上传参数，可以修改header信息。

###### 1.2.1 使用GET方式抓取数据

​	在浏览器中，较常使用的两种操作：

​	**Ctrl + U**：查看网页源码界面，可以查看网页的标题等。

​		![1556448918219](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556448918219.png)

​	**F12**：查看网页的动态操作，如请求，包等等，如下图。

​		![1556449083679](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556449083679.png)

```python
import requests
url = "http://www.cntour.cn"
strhtml = requests.get(url)	# 直接通过resquest库中的get方法访问
print(strhtml.text)			# .text方法返回网页源码的信息
```

###### 1.2.2 使用POST方式抓取数据

​	POST方式需要配合浏览器进行操作。

​	例如，访问百度翻译，并输入汉字进行翻译，这个就是使用POST的形式。

![1556449929321](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556449929321.png)

​	输入文字点翻译

![1556450023243](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556450023243.png)

​	点击preview查看获取的信息的格式

![1556450115514](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556450115514.png)

​	这些就是我们查找到的结果。

下面是使用POST方式使用有道翻译的代码，postf方式需要构建请求头，所以需要额外的信息。

![1556450442153](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556450442153.png)

![1556450538608](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556450538608.png)

```python
import requests
import json
def get_translate_data(word = None):
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
    # data省略了较多信息，但是这两个不能省
    data = {'i': '中国',
    'doctype': 'json',
    }
    # 这里是request header的信息，需要这两个作为参数，传给服务器才可以
    headers = {
        'Host': 'fanyi.youdao.com',
        'Origin': 'http://fanyi.youdao.com',
        'Referer': 'http://fanyi.youdao.com/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36',
        'X-Requested-With': 'XMLHttpRequest'
    }
    req = requests.post(url, data=data, headers=headers)
    # 使用utf-8格式解码
    html = req.content.decode('utf-8')
    data = json.loads(html)
    print("翻译结果：{}".format((data['translateResult'][0][0]['tgt'])))

if __name__ == "__main__":
    get_translate_data()
```

##### 1.3 使用Beautiful Soup解析网页并清洗

​	由于我们已经可以抓取网页上的源码，所以需要通过第三方库解析网页源码。

​	例如抽取网页的标题。通过右键标题并点击检查，可以找到标题所对应的源码。

![1556451977689](https://github.com/lihao056/spider/blob/master/book_learning/picture/picture/1556451977689.png)

![1556452106063](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556452106063.png)

​	在源码处，右键后选择Copy至Copy selector，可获得当前标题的路径

![1556452217680](https://github.com/lihao056/spider/blob/master/book_learning/picture/1556452217680.png)

​	

```python
url = "http://www.cntour.cn"
strhtml = requests.get(url)
# HTML文档会转换成Unicode格式，通过Beautiful格式的解析器进行解析
soup = BeautifulSoup(strhtml.text, 'lxml')
'''
源码路径：
#main > div > div.mtop.firstMod.clearfix > div.centerBox > ul.newsList > li:nth-child(1) > a	
将:后的文字删除，即可获得路径，通过soup包即可获得。
'''
data = soup.select('#main > div > div.mtop.firstMod.clearfix > div.centerBox > ul.newsList > li > a')
print(data)
# 下面是清洗数据部分，使用select是获取了目标的HTML代码，还没有把数据提取出来。
for item in data:
    result = {
        # 标题在a标签中，提取标签正文用get_text()方法
        'title':item.get_text(),
        # 提取href属性使用get方法，
        'link':item.get('href'),
        'ID':re.findall('\d+', item.get('href'))
    }
    print(result)
```

##### 1.4 爬虫攻防战

​	在反爬虫阶段可以使用如下的一些方法。

​	1.在request headers中构建自己的请求头。

​		如同post部分代码中创建的一样。

​		get方法也可以创建，使用的方法如下

```python
# 使用headers头信息
headers = {
        'Host': 'fanyi.youdao.com',
        'Origin': 'http://fanyi.youdao.com',
        'Referer': 'http://fanyi.youdao.com/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36',
        'X-Requested-With': 'XMLHttpRequest'
    } 
response = requests.get(url, headers = headers)
```

​	2.在爬虫阶段增设延时功能，

​		原因是在一个ip中出现频繁访问的情况是反人类的，所以需要增加访问时间降低频率。

```python
time.sleep(3)		# 延时3秒
```

​	3. 构建自己的代理池，防止ip被封锁

```python
proxies = {
    "http": "http://10.110.1.10:3128",
    "https": "http://10.110.1.10:1080"
}
response = request.get(url, proxies=proxies)
```

